{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Building and Training Sequence-to-Sequence Models for Language Processing with PyTorch and fastai**"
      ],
      "metadata": {
        "id": "d4VgykHSkoKG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "0BAhp755mLuc"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries from fastai for text processing\n",
        "from fastai.text.all import *\n",
        "\n",
        "# Download and prepare the Human Numbers dataset\n",
        "path = untar_data(URLs.HUMAN_NUMBERS)\n",
        "Path.BASE_PATH = path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and prepare data\n",
        "def load_data(path):\n",
        "    lines = L()\n",
        "    # Read lines from train and validation files\n",
        "    for file in ['train.txt', 'valid.txt']:\n",
        "        with open(path/file) as f: lines += L(*f.readlines())\n",
        "    # Create a single text stream and tokenize\n",
        "    text = ' . '.join([line.strip() for line in lines])\n",
        "    tokens = text.split(' ')\n",
        "    return tokens\n",
        "\n",
        "tokens = load_data(path)"
      ],
      "metadata": {
        "id": "GsYFaRhMdsxy"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a vocabulary and numericalize tokens\n",
        "vocab = L(*tokens).unique()\n",
        "word2idx = {word: i for i, word in enumerate(vocab)}\n",
        "nums = L(word2idx[token] for token in tokens)\n",
        "\n",
        "# Prepare sequences for the model\n",
        "def create_sequences(nums):\n",
        "    seq_len = 3\n",
        "    sequences = L((tensor(nums[i:i+seq_len]), nums[i+seq_len]) for i in range(0, len(nums)-seq_len))\n",
        "    return sequences\n",
        "\n",
        "sequences = create_sequences(nums)"
      ],
      "metadata": {
        "id": "cM4cmBnkdte3"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and validation sets and create DataLoader\n",
        "def split_data(sequences, train_pct=0.8):\n",
        "    cut = int(len(sequences) * train_pct)\n",
        "    return sequences[:cut], sequences[cut:]\n",
        "\n",
        "train_seqs, valid_seqs = split_data(sequences)\n",
        "\n",
        "bs = 64\n",
        "dls = DataLoaders.from_dsets(train_seqs, valid_seqs, bs=bs, shuffle=False)"
      ],
      "metadata": {
        "id": "zH77haladzTn"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple RNN model architecture\n",
        "class SimpleRNN(Module):\n",
        "    def __init__(self, vocab_sz, n_hidden):\n",
        "        self.i_h = nn.Embedding(vocab_sz, n_hidden)  # Input to hidden\n",
        "        self.h_h = nn.Linear(n_hidden, n_hidden)     # Hidden to hidden\n",
        "        self.h_o = nn.Linear(n_hidden, vocab_sz)     # Hidden to output\n",
        "        self.h = 0                                   # Initialize hidden state\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(3):  # Process 3 inputs from the sequence\n",
        "            self.h = self.h + self.i_h(x[:, i])\n",
        "            self.h = F.relu(self.h_h(self.h))\n",
        "        return self.h_o(self.h)"
      ],
      "metadata": {
        "id": "TZwiR7dpd1ys"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "# Define the first simple language model\n",
        "class LMModel1(Module):\n",
        "    def __init__(self, vocab_sz, n_hidden):\n",
        "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "        self.h_h = nn.Linear(n_hidden, n_hidden)\n",
        "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = F.relu(self.h_h(self.i_h(x[:,0])))\n",
        "        h = h + self.i_h(x[:,1])\n",
        "        h = F.relu(self.h_h(h))\n",
        "        h = h + self.i_h(x[:,2])\n",
        "        return self.h_o(F.relu(self.h_h(h)))"
      ],
      "metadata": {
        "id": "6c0V-JVmnZYk"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Our First Recurrent Neural Network\n",
        "class LMModel2(Module):\n",
        "    def __init__(self, vocab_sz, n_hidden):\n",
        "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "        self.h_h = nn.Linear(n_hidden, n_hidden)\n",
        "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = 0\n",
        "        for i in range(3):\n",
        "            h = h + self.i_h(x[:,i])\n",
        "            h = F.relu(self.h_h(h))\n",
        "        return self.h_o(h)\n"
      ],
      "metadata": {
        "id": "Z7ASAx9RaK71"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LMModel3(Module):\n",
        "    def __init__(self, vocab_sz, n_hidden):\n",
        "        self.i_h = nn.Embedding(vocab_sz, n_hidden)  # Input to hidden layer\n",
        "        self.h_h = nn.Linear(n_hidden, n_hidden)     # Hidden to hidden layer\n",
        "        self.h_o = nn.Linear(n_hidden, vocab_sz)    # Hidden to output layer\n",
        "        self.n_hidden = n_hidden\n",
        "        self.h = None  # Initialize hidden state as None\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.h is None or self.h.size(0) != x.size(0):\n",
        "            # Initialize hidden state to zeros if it is None or batch size has changed\n",
        "            self.h = torch.zeros(x.size(0), self.n_hidden, device=x.device)\n",
        "        for i in range(3):\n",
        "            self.h = self.h + self.i_h(x[:,i])\n",
        "            self.h = F.relu(self.h_h(self.h))\n",
        "        out = self.h_o(self.h)\n",
        "        self.h = self.h.detach()  # Detach to prevent backprop through the entire history\n",
        "        return out\n",
        "\n",
        "    def reset(self):\n",
        "        self.h = None  # Reset hidden state to None"
      ],
      "metadata": {
        "id": "24J4ibFvd8xc"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Callback to reset model state at the beginning of training and validation\n",
        "class ModelResetter(Callback):\n",
        "    def before_train(self): self.model.reset()\n",
        "    def before_validate(self): self.model.reset()\n",
        "\n",
        "# Training the improved RNN\n",
        "learn = Learner(dls, LMModel3(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy, cbs=ModelResetter)\n",
        "learn.fit_one_cycle(10, 3e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "F2K4wJ7-d-pm",
        "outputId": "2e8a8c1d-3a9b-4836-9648-5da9770fb71c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.335075</td>\n",
              "      <td>1.789224</td>\n",
              "      <td>0.484824</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.036080</td>\n",
              "      <td>1.561900</td>\n",
              "      <td>0.502258</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.025811</td>\n",
              "      <td>1.718748</td>\n",
              "      <td>0.507964</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.003425</td>\n",
              "      <td>1.978140</td>\n",
              "      <td>0.514779</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.987475</td>\n",
              "      <td>2.286434</td>\n",
              "      <td>0.520168</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.026209</td>\n",
              "      <td>2.152428</td>\n",
              "      <td>0.507172</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.992380</td>\n",
              "      <td>2.056885</td>\n",
              "      <td>0.530866</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.035084</td>\n",
              "      <td>2.183841</td>\n",
              "      <td>0.522070</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.096464</td>\n",
              "      <td>1.738006</td>\n",
              "      <td>0.525636</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.217491</td>\n",
              "      <td>1.404073</td>\n",
              "      <td>0.512560</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def group_chunks(ds, bs):\n",
        "    m = len(ds) // bs\n",
        "    new_ds = L()\n",
        "    for i in range(m): new_ds += L(ds[i + m*j] for j in range(bs))\n",
        "    return new_ds"
      ],
      "metadata": {
        "id": "g0t65CxggH2T"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sl = 16\n",
        "seqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1]))\n",
        "         for i in range(0,len(nums)-sl-1,sl))\n",
        "cut = int(len(seqs) * 0.8)\n",
        "dls = DataLoaders.from_dsets(group_chunks(seqs[:cut], bs),\n",
        "                             group_chunks(seqs[cut:], bs),\n",
        "                             bs=bs, drop_last=True, shuffle=False)"
      ],
      "metadata": {
        "id": "ZqfpofjjfqUB"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LMModel4(Module):\n",
        "    def __init__(self, vocab_sz, n_hidden):\n",
        "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "        self.h_h = nn.Linear(n_hidden, n_hidden)\n",
        "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
        "        self.h = 0  # This will be updated to handle batch sizes dynamically\n",
        "\n",
        "    def forward(self, x):\n",
        "        outs = []\n",
        "        for i in range(x.size(1)):  # Assuming x is of shape (batch_size, sequence_length)\n",
        "            self.h = self.h + self.i_h(x[:,i])\n",
        "            self.h = F.relu(self.h_h(self.h))\n",
        "            outs.append(self.h_o(self.h))\n",
        "        self.h = self.h.detach()\n",
        "        return torch.stack(outs, dim=1)\n",
        "\n",
        "    def reset(self):\n",
        "        self.h = 0  # Resets state for a new sequence\n",
        "\n",
        "\n",
        "def loss_func(inp, targ):\n",
        "    return F.cross_entropy(inp.view(-1, len(vocab)), targ.view(-1))\n",
        "learn = Learner(dls, LMModel4(len(vocab), 64), loss_func=loss_func, metrics=accuracy, cbs=ModelResetter)\n",
        "learn.fit_one_cycle(15, 3e-3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "rYr-WbIce-N5",
        "outputId": "b26bb879-c2b1-4da4-9871-dc9aa3e05047"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.329795</td>\n",
              "      <td>3.128267</td>\n",
              "      <td>0.180094</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.393921</td>\n",
              "      <td>1.968232</td>\n",
              "      <td>0.469727</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.778201</td>\n",
              "      <td>1.871350</td>\n",
              "      <td>0.457031</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.504304</td>\n",
              "      <td>1.817366</td>\n",
              "      <td>0.489502</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.332533</td>\n",
              "      <td>1.851684</td>\n",
              "      <td>0.526204</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.195112</td>\n",
              "      <td>1.843094</td>\n",
              "      <td>0.533203</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.082083</td>\n",
              "      <td>2.012559</td>\n",
              "      <td>0.581868</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.001093</td>\n",
              "      <td>2.035439</td>\n",
              "      <td>0.586670</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.931485</td>\n",
              "      <td>2.102063</td>\n",
              "      <td>0.597819</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.869821</td>\n",
              "      <td>2.269284</td>\n",
              "      <td>0.625977</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.827731</td>\n",
              "      <td>2.207064</td>\n",
              "      <td>0.592611</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.798711</td>\n",
              "      <td>2.064577</td>\n",
              "      <td>0.631917</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.769095</td>\n",
              "      <td>2.097769</td>\n",
              "      <td>0.638672</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.742992</td>\n",
              "      <td>2.191283</td>\n",
              "      <td>0.615072</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.726674</td>\n",
              "      <td>2.180404</td>\n",
              "      <td>0.615072</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LSTM**"
      ],
      "metadata": {
        "id": "wKGjIKwcgnqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMCell(Module):\n",
        "    def __init__(self, ni, nh):\n",
        "        self.ih = nn.Linear(ni,4*nh)\n",
        "        self.hh = nn.Linear(nh,4*nh)\n",
        "\n",
        "    def forward(self, input, state):\n",
        "        h,c = state\n",
        "        # One big multiplication for all the gates is better than 4 smaller ones\n",
        "        gates = (self.ih(input) + self.hh(h)).chunk(4, 1)\n",
        "        ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3])\n",
        "        cellgate = gates[3].tanh()\n",
        "\n",
        "        c = (forgetgate*c) + (ingate*cellgate)\n",
        "        h = outgate * c.tanh()\n",
        "        return h, (h,c)"
      ],
      "metadata": {
        "id": "wTFcjKbXemL9"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.arange(0,10); t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLCrZeWmgv_9",
        "outputId": "c29241ee-4977-4f87-fee2-f2378b661420"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t.chunk(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeZ4tLS0hLjK",
        "outputId": "b96ecc7d-a9ff-417c-b668-191cc99a257a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0, 1, 2, 3, 4]), tensor([5, 6, 7, 8, 9]))"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LMModel6(Module):\n",
        "    def __init__(self, vocab_sz, n_hidden, n_layers):\n",
        "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n",
        "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
        "        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n",
        "\n",
        "    def forward(self, x):\n",
        "        res,h = self.rnn(self.i_h(x), self.h)\n",
        "        self.h = [h_.detach() for h_ in h]\n",
        "        return self.h_o(res)\n",
        "\n",
        "    def reset(self):\n",
        "        for h in self.h: h.zero_()\n",
        "\n",
        "\n",
        "learn = Learner(dls, LMModel6(len(vocab), 64, 2),\n",
        "                loss_func=CrossEntropyLossFlat(),\n",
        "                metrics=accuracy, cbs=ModelResetter)\n",
        "learn.fit_one_cycle(15, 1e-2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "b8J9YHgghN84",
        "outputId": "b9f6c1a4-faf3-432a-f1c0-395e3555eb9e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.021744</td>\n",
              "      <td>2.702758</td>\n",
              "      <td>0.318115</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.220380</td>\n",
              "      <td>2.398314</td>\n",
              "      <td>0.265544</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.619505</td>\n",
              "      <td>1.872986</td>\n",
              "      <td>0.479492</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.289589</td>\n",
              "      <td>2.184089</td>\n",
              "      <td>0.523519</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.015464</td>\n",
              "      <td>2.098790</td>\n",
              "      <td>0.587321</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.759935</td>\n",
              "      <td>1.984510</td>\n",
              "      <td>0.620443</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.545575</td>\n",
              "      <td>1.749310</td>\n",
              "      <td>0.665609</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.356040</td>\n",
              "      <td>1.663230</td>\n",
              "      <td>0.688151</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.233045</td>\n",
              "      <td>1.645964</td>\n",
              "      <td>0.732992</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.145595</td>\n",
              "      <td>1.657074</td>\n",
              "      <td>0.735352</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.091537</td>\n",
              "      <td>1.630050</td>\n",
              "      <td>0.749268</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.059719</td>\n",
              "      <td>1.757318</td>\n",
              "      <td>0.745199</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.042686</td>\n",
              "      <td>1.701665</td>\n",
              "      <td>0.746582</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.034011</td>\n",
              "      <td>1.731097</td>\n",
              "      <td>0.746257</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.030111</td>\n",
              "      <td>1.731273</td>\n",
              "      <td>0.746908</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Regularizing an LSTM**"
      ],
      "metadata": {
        "id": "utj9TbLwhoP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dropout(Module):\n",
        "    def __init__(self, p): self.p = p\n",
        "    def forward(self, x):\n",
        "        if not self.training: return x\n",
        "        mask = x.new(*x.shape).bernoulli_(1-p)\n",
        "        return x * mask.div_(1-p)"
      ],
      "metadata": {
        "id": "2fmznpHShRzE"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LMModel7(Module):\n",
        "    def __init__(self, vocab_sz, n_hidden, n_layers, p):\n",
        "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n",
        "        self.drop = nn.Dropout(p)\n",
        "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
        "        self.h_o.weight = self.i_h.weight\n",
        "        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n",
        "\n",
        "    def forward(self, x):\n",
        "        raw,h = self.rnn(self.i_h(x), self.h)\n",
        "        out = self.drop(raw)\n",
        "        self.h = [h_.detach() for h_ in h]\n",
        "        return self.h_o(out),raw,out\n",
        "\n",
        "    def reset(self):\n",
        "        for h in self.h: h.zero_()"
      ],
      "metadata": {
        "id": "-0oU9hzPheZU"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn = Learner(dls, LMModel7(len(vocab), 64, 2, 0.5),\n",
        "                loss_func=CrossEntropyLossFlat(), metrics=accuracy,\n",
        "                cbs=[ModelResetter, RNNRegularizer(alpha=2, beta=1)])"
      ],
      "metadata": {
        "id": "UnS31Dhfhgtw"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn = TextLearner(dls, LMModel7(len(vocab), 64, 2, 0.4),\n",
        "                    loss_func=CrossEntropyLossFlat(), metrics=accuracy)"
      ],
      "metadata": {
        "id": "KKfVUinmhjVZ"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.fit_one_cycle(15, 1e-2, wd=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "GHnQWIQehk8M",
        "outputId": "cca92c06-75fb-456d-9a44-72831b27714d"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2.610863</td>\n",
              "      <td>2.077192</td>\n",
              "      <td>0.455322</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.597773</td>\n",
              "      <td>1.432384</td>\n",
              "      <td>0.596598</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.846239</td>\n",
              "      <td>0.894360</td>\n",
              "      <td>0.802409</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.413939</td>\n",
              "      <td>0.826460</td>\n",
              "      <td>0.839518</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.211952</td>\n",
              "      <td>0.700753</td>\n",
              "      <td>0.848307</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.114560</td>\n",
              "      <td>0.652205</td>\n",
              "      <td>0.864502</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.068979</td>\n",
              "      <td>0.537934</td>\n",
              "      <td>0.879150</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.047844</td>\n",
              "      <td>0.621620</td>\n",
              "      <td>0.876628</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.038060</td>\n",
              "      <td>0.594036</td>\n",
              "      <td>0.877197</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.028884</td>\n",
              "      <td>0.687566</td>\n",
              "      <td>0.871012</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.023071</td>\n",
              "      <td>0.672428</td>\n",
              "      <td>0.874268</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.019302</td>\n",
              "      <td>0.608040</td>\n",
              "      <td>0.876628</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.015850</td>\n",
              "      <td>0.566805</td>\n",
              "      <td>0.881917</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.013442</td>\n",
              "      <td>0.543176</td>\n",
              "      <td>0.885498</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.012276</td>\n",
              "      <td>0.547570</td>\n",
              "      <td>0.884847</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}